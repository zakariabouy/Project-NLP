{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf899e45",
   "metadata": {},
   "source": [
    "* * *\n",
    "<pre> INSEA                <i> Projet Statistiques Multivariées - 2025 </i></pre>\n",
    "* * *\n",
    "\n",
    "\n",
    "<pre align=\"left\"> Mardi 4 Novembre  2025             <i> Author: Hicham Janati </i></pre>\n",
    "* * *\n",
    "\n",
    "Consignes:\n",
    "----------\n",
    "- Deadline: *Dimanche 7 Décembre 23h59* | tout retard d'1h = -1\n",
    "- Tout travail doit être effectué par groupe de deux sans exception.\n",
    "- Il faut nommer le fichier avec le numéro du groupe seulement `groupe_X.ipynb` pas de noms. \n",
    "- PAS DE PDF NI DE FICHIER WORD. UN NOTEBOOK UNIQUEMENT.\n",
    "- Les noms des membres du groupe sont à mentionner en modifiant la ligne suivante:\n",
    "\n",
    "Membre 1:\n",
    "\n",
    "Membre 2:\n",
    "\n",
    "- Ce notebook contient 15 questions.\n",
    "- Il faut executer tout le notebook (Run all cells) avant de l'envoyer: vérifier que les cellules sont exécutées dans le bon ordre et qu'il n'y a pas d'erreur, je ne vais pas débugger votre code. \n",
    "- Le code doit être propre et lisible et surtout comprendre ce qu'il fait par tous les membres du groupe.\n",
    "- LIVRAISON UNIQUEMENT SUR DROPBOX (TOUT FICHIER ENVOYE PAR MAIL NE SERA PAS OUVERT):\n",
    "\n",
    "https://www.dropbox.com/request/bs2Tafzm0Bzr7rroAGtc\n",
    "\n",
    "Vous pouvez effectuer plusieurs uploads, seule la date la plus récente sera corrigée.\n",
    "\n",
    "- Le but n'est pas de produire du code qui marche: le but est de comprendre et maîtriser les mécanismes derrière et savoir utiliser quoi et quand. Ce notebook est loin d'être self-contained, cherchez, lisez et apprenez à devenir auto-didacte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ebe82",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f292b97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35d2f782",
   "metadata": {},
   "source": [
    "# Partie 1: Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6afb9",
   "metadata": {},
   "source": [
    "Les données sont coupés en deux: train et test. Il s'agit d'une liste d'SMS avec leur labels (Y) spam / non spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803e938",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/X_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4040571986.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/X_train.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/X_test.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/Y_train.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/X_train.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "X_train = np.load(\"data/X_train.npy\", allow_pickle=True)\n",
    "X_test = np.load(\"data/X_test.npy\", allow_pickle=True)\n",
    "Y_train = np.load(\"data/Y_train.npy\", allow_pickle=True)\n",
    "Y_test = np.load(\"data/Y_test.npy\", allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7e12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Play w computer? Aiyah i tok 2 u lor?',\n",
       "        \"Well there's not a lot of things happening in Lindsay on New years *sighs* Some bars in Ptbo and the blue heron has something going\",\n",
       "        'Dear are you angry i was busy dear',\n",
       "        \"Carlos'll be here in a minute if you still need to buy\"],\n",
       "       dtype=object),\n",
       " array([0, 0, 0, 0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:4], Y_train[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5e565",
   "metadata": {},
   "source": [
    "On rappelle que les données de `test` doivent être utilisées pour évaluer le modèle final comme si elles étaient nouvelles en production -- non disponibles à l'entraînement -- toute opération d'apprentissage (y compris la validation croisée) doit être faite sur les données `train` uniquement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3366050",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "En utilisant ```CountVectorizer``` de ```scikit-learn``` transformez les données. Quelle est l'utilité de cette opération ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cb7ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn.__check_build._check_build'\n___________________________________________________________________________\nContents of c:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\sklearn\\__check_build:\nmeson.build               _check_build.cp311-win_amd64.lib_check_build.cp311-win_amd64.pyd\n_check_build.pyx          __init__.py               __pycache__\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it. For detailed instructions, see:\nhttps://scikit-learn.org/dev/developers/advanced_installation.html#building-from-source\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\sklearn\\__check_build\\__init__.py:52\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_check_build\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_build  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn.__check_build._check_build'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\sklearn\\__init__.py:69\u001b[39m\n\u001b[32m     60\u001b[39m os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mKMP_INIT_AT_FORK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mFALSE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\sklearn\\__check_build\\__init__.py:54\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_check_build\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_build  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[43mraise_build_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\sklearn\\__check_build\\__init__.py:35\u001b[39m, in \u001b[36mraise_build_error\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     34\u001b[39m             dir_content.append(filename + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     36\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"%s\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m___________________________________________________________________________\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03mContents of %s:\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m%s\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m___________________________________________________________________________\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03mIt seems that scikit-learn has not been built correctly.\u001b[39;00m\n\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m \u001b[33;03mIf you have installed scikit-learn from source, please do not forget\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[33;03mto build the package before using it. For detailed instructions, see:\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33;03mhttps://scikit-learn.org/dev/developers/advanced_installation.html#building-from-source\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[33;03m%s\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m         % (e, local_dir, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(dir_content).strip(), msg)\n\u001b[32m     48\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: No module named 'sklearn.__check_build._check_build'\n___________________________________________________________________________\nContents of c:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\sklearn\\__check_build:\nmeson.build               _check_build.cp311-win_amd64.lib_check_build.cp311-win_amd64.pyd\n_check_build.pyx          __init__.py               __pycache__\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it. For detailed instructions, see:\nhttps://scikit-learn.org/dev/developers/advanced_installation.html#building-from-source\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a093b",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "On modélise les données de chaque classe par une loi multinomiale. En utilisant ```MultinomialNB``` de scikit-learn, fittez un modèle et évaluez sa performance en calculant les scores de précision et de rappel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509eb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b95a3d7d",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "Implémentez votre propre modèle naive bayes multinomial en utilisant numpy. Comparez avec scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dfadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3034ba",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "Afin de comparer avec d'autres modèles qui s'attendent à des inputs continus, on peut utiliser des fréquences de mots au lieu du nombre brut. Appliquez la transformation `TfIdf` de scikit-learn sur les données. Fittez des modèles différents: LDA, régression logistique, QDA et SVM. Essayez de battre la performance du MulinomialNB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8d576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29b781aa",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "Visualisez la PCA et tSNE des données transformées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86c4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c872a753",
   "metadata": {},
   "source": [
    "# Partie II: Introduction au NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47767fe",
   "metadata": {},
   "source": [
    "Dans cette partie on utilise des transformations plus avancées basées sur la factorisation matricielle. L'un des premiers modèles à grand succès pour représenter les mots est `Word2Vec`. Étant donné un vocabulaire de taille V, on peut représenter un input par un vecteur binaire de taille V (one-hot encoding). Par exemple si le vocabulaire est\n",
    "```[\"rouge\", \"chat\", \"souris\", \"courir\", \"mange\", \"ordinateur\", \"la\", \"le\", ...]``` \n",
    "Alors on peut représenter chaque mot par son vecteur one-hot de taille V:\n",
    "- \"chat\" -> [0, 1, 0, 0, 0, ...]\n",
    "- \"ordinateur\" -> [0, 0, 0, 0, 0, 1, 0, ...]\n",
    "\n",
    "La phrase \"le chat mange la souris\" peut être représentée par [0, 1, 1, 0, 1, 0, 1, 1, ...].\n",
    "\n",
    "On construit un réseau de neurones simple capable de prédire le mot manquant au milieu d'une phrase avec des données comme:\n",
    "- \"le chat la souris\" -> \"mange\"\n",
    "- \"la matrice est pas inversible\" -> \"n'\" \n",
    "...\n",
    "Ainsi, l'input du réseau doit être un vecteur de taille V. L'output doit être également **un vecteur de probabilités** de taille V. On prédit le mot avec la plus grande probabilité.\n",
    "\n",
    "On considère un réseau de neurones à une couche cachée de dimension M (M neurones). Le réseau de neurones doit en sortie prédire un mot caché. Soit $x \\in \\mathbb R^V$ un one-hot vecteur. Le réseau de neurones peut être défini formellement comme suit. Soit $W_{in} \\in \\mathbb{R}^{V \\times M}$ et $W_{out} \\in \\mathbb{R}^{M \\times V}$. \n",
    "\n",
    "$$ g(x) = W_{out}(W_{in}x) \\in \\mathbb R^{V} $$\n",
    "\n",
    "Pour obtenir un vecteur output positif et sommant à 1 (des probabilités sur les mots), on applique la fonction softmax:\n",
    "$$ softmax: z \\in \\mathbb R^V \\mapsto \\left[\\frac{e^{z_1}}{\\sum_j e^{z_j}}, \\dots, \\frac{e^{z_V}}{\\sum_j e^{z_j}}  \\right]^\\top$$\n",
    "\n",
    "Ainsi on définit: $ neuralnet(x) = softmax(g(x))$ et on entraîne le modèle avec la loss cross-entropy.\n",
    " \n",
    "Après avoir entraîné le modèle, les ligne de la matrice $W_{in}$ sont les embeddings des mots du vocabulaire.\n",
    "\n",
    "\n",
    "On commence par télécharger le modèle word2vec (peut prendre du temps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3818bbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy.spatial._ckdtree'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapi\u001b[39;00m\n\u001b[32m      3\u001b[39m word2vec = api.load(\u001b[33m\"\u001b[39m\u001b[33mword2vec-google-news-300\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.4.0\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\gensim\\matutils.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlapack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\scipy\\stats\\__init__.py:626\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    621\u001b[39m \n\u001b[32m    622\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    625\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance_matrix\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m milp, LinearConstraint\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, _get_nan,\n\u001b[32m     44\u001b[39m                               _rename_parameter, _contains_nan,\n\u001b[32m     45\u001b[39m                               normalize_axis_index, np_vecdot, AxisError)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\scipy\\spatial\\__init__.py:110\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m=============================================================\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \u001b[33;03m   QhullError\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_kdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_qhull\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CE PC\\Project-NLP-2025\\.venv\\Lib\\site-packages\\scipy\\spatial\\_kdtree.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright Anne M. Archibald 2008\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Released under the scipy license\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[32m      6\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mminkowski_distance_p\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mminkowski_distance\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mdistance_matrix\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mRectangle\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mKDTree\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mminkowski_distance_p\u001b[39m(x, y, p=\u001b[32m2\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'scipy.spatial._ckdtree'"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbabe3a",
   "metadata": {},
   "source": [
    "word2vec est n'est qu'un dictionnaire spécial \"mot\" -> vecteur:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b64da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec[\"computer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcecdc8",
   "metadata": {},
   "source": [
    "On peut obtenir l'embedding de plusieurs mots directement dans une liste. L'embedding d'une phrase est souvent défini comme la moyenne des embeddings de ses mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec[[\"computer\", \"mouse\", \"keyboard\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a273ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec[[\"I\", \"love\", \"computers\"]].mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6fc08",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Pour appliquer word2vec aux SMS, il faut d'abord les découper en mots. Implémentez une fonction naïve qui transforme un SMS en liste de mots ou _tokens_. C'est ce qu'on appelle un tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a05f7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f73641d4",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "On peut vérifier si un mot fait partie du vocabulaire avec le test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32398c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"computer\" in word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9090ca6",
   "metadata": {},
   "source": [
    "Proposez une solution pour appliquer word2vec aux données SMS. Est-il meilleur comme embedding comparé au `CountVectorizer` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e9ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be090b85",
   "metadata": {},
   "source": [
    "`Word2vec` (2013) est l'un des premiers modèles de représentation vectorielle du langage naturel. L'une de ses limitations principales est le fait de ne pas pouvoir représenter des mots (ou bouts de mots) non-vus lors de l'entraînement. Quelques années plus tard, plusieurs avancées ont été introduites pour y remédier:\n",
    "\n",
    "### 1) Améliorer la tokenization:\n",
    "1. Au lieu de considérer un vocabulaire avec des mots, on considère un vocabulaire avec les mots, et les bouts de mots (ngrams):\n",
    "\n",
    "    \"diagonalisation matricielle\" -> \"diagonal\", \"isation\", \" \", \"matric\", \"ielle\". \n",
    "\n",
    "    Ceci est utilisé dans le modèle `fastText` (2016) dont l'architecture est similaire à celle de Word2Vec.\n",
    "\n",
    "2. WordPiece: approche bottom-up où le vocabulaire des tokens est construit en mergeant les caractères (a-Z, 0-9, symboles et ponctuation) + Ajouter des tokens spéciaux \"[UNK]\" (unknow) pour gérer les tokens non-vus, \"[SEP]\" (séparateur entre phrases ou textes), \"[MASK]\" (token qui veut dire \"ce mot est à prédire\") etc. Ce tokenizer est utilisé dans l'un des premiers modèles basés sur l'architecture Transformer (BERT) (2018).\n",
    "\n",
    "3. Byte-pair encoding (BPE): similaire à WordPiece mais à l'échelle du byte.  Ceci permet d'encoder n'importe quel string dans n'importe quelle langue: mots rares, typos, emojis, kanji... C'est le tokenizer adopté par l'un des fameux modèles de traduction à l'époque (Neural Machine Translation -- NMT 2015) pour traduire les mots rares; et ensuite par les modèles GPT (2018). Avec BPE, on n'a plus besoin du token spécial _unknown_ [UNK]. Vous pouvez voir comment le texte est tokenisé par GPT en visitant https://platform.openai.com/tokenizer. \n",
    "\n",
    "### 2) Améliorer l'architecture\n",
    "\n",
    "1. Limites de Word2Vec et companie:\n",
    "\n",
    "La nature des données de langage est séquentielle (comme les séries temporelles, mot après mot..), or les modèles comme Word2Vec et ses variantes (Fasttext, GloVe) ne prennent pas en considération l'ordre des mots: ils sont appliqués à des _bag of words_. En plus, même si ses modèles utilisent le contexte voisin, après l'entraînement, l'embedding d'un mot est le même quelque soit son contexte ce qui limite la compréhension des différentes nuances d'un même mot. \n",
    "\n",
    "2. Recurrent Neural Networks:\n",
    "\n",
    "L'architecture des réseaux de neurones récurrents (RNN) et ses variantes (LSTM, GRU) développés principalement dans les années 1990 sont revisités en (2013-2020) après le succès du deep learning (2012+) avec la réalisation de la grande puissance de calcul des GPUs. Ils permettent de prendre en considération l'aspect temporel des données mais restent difficiles à entraîner (nécessitent beaucoup de temps + instabilité des gradients). \n",
    "\n",
    "3. Transformers (en très bref):\n",
    "\n",
    "En 2017, l'architecture des Transformers est introduite. Son avantage principal est son formalisme matriciel qui est beaucoup plus parallélisable que les RNNs, et donc permet d'entraîner des modèles plus grands sur des données beaucoup plus grandes. Prenons le cas d'une architecture générative (GPT). Un transformer définit une structure appelée _Self-Attention Head_ qui fonctionne comme suit.\n",
    "Étant donnés les embeddings (statiques comme ceux de Word2Vec) des tokens d'une phrase, le mécanisme d'attention consiste à calculer des scores de similarités (produits scalaires) entre chaque paire d'embeddings, ceci permet d'obtenir une matrice de scores \"d'attention\" entre les tokens.\n",
    "\n",
    "Par exemple, pour une phrase \"Il me faut un tapis pour la souris Logitec\", l'embedding du mot \"souris\" sera pondéré par ses scores de similarité avec chacun des autres mots [\"Il\", \"me\", \"faut\", \"un\", \"tapis\", \"pour\", \"la\", \"logitec\"]. Ainsi, l'embedding d'un même mot dépend du contexte. Ceci permettra de distinguer la souris (animal) de la souris d'ordinateur.\n",
    "\n",
    " Ceci reste une explication très simpliste du mécanisme d'attention, pour visualiser le concept, regardez la vidéo de 3b1b https://www.youtube.com/watch?v=eMlx5fFNoYc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5a415",
   "metadata": {},
   "source": [
    "## III Partie 3 - Embeddings basés sur les transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf35bb",
   "metadata": {},
   "source": [
    "Dans cette partie, on passe à l'artillerie lourde: embeddings basés sur les transformers. On utilise ceux de BERT plutôt que GPT car BERT a été conçu pour la classification supervisée alors que GPT pour la génération de texte: l'embedding de BERT est plus pertinent pour notre problématique.\n",
    "\n",
    "On commence par tokeniser les données. Voici ce que donne le tokenizer avec une phrase par exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4469cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71330dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hey, are you free tomorrow at 8 ? gotta catch up bud.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2aec4",
   "metadata": {},
   "source": [
    "Le modèle BERT en revanche ne s'attend pas à des tokens en texte, mais aux token_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hey, are you free tomorrow at 8 ? gotta catch up bud\"\n",
    "token_ids = tokenizer(text, return_tensors=\"pt\")\n",
    "token_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5213c82",
   "metadata": {},
   "source": [
    "On fait appel ensuite au modèle BERT. On peut voir les composants de son architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()\n",
    "bert_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07b8ac",
   "metadata": {},
   "source": [
    "On applique le modèle aux token_ids sans calcul de gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482197eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(**token_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6611fe03",
   "metadata": {},
   "source": [
    "On peut accéder aux embeddings en cherchant la dernière couche cachée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86436f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389bfc99",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Expliquez la dimension obtenue de ces embeddings. On souhaite obtenir les embeddings de toutes les données directement. Appliquez la tokenization à la liste de textes suivante. Quel problème se pose ? Comment y remédier ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80fc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Hey, are you free tomorrow at 8 ? gotta catch up bud.\", \"Sure thing, let's go.\"]\n",
    "token_ids = tokenizer(texts, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfec6a8",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Après avoir fixé le problème, analysez les tokens obtenus et le `attention_mask` de l'output. Comment peut-on savoir où le `padding` a été appliqué ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669b170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6225300c",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "L'embedding d'une phrase est la moyenne des embeddings de ses tokens. Appliquez BERT à une liste des donnés spams (X_train[:10]). Calculez le temps pris par cette opération en utilisant la librairie `time`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4020d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "713b2079",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "Avec une GPU, ce calcul peut être jusqu'à 100x fois plus rapide. On vous fournit les embeddings des données `X_train` et `X_test` calculés avec BERT. Refaire l'étape de visualisation et classification avec ces embeddings pour comparer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train_bert = np.load(\"data/embeddings_train.npy\")\n",
    "X_test_bert = np.load(\"data/embeddings_test.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b87e89",
   "metadata": {},
   "source": [
    "# IV Partie 4: Data drift \n",
    "En production, les données nouvelles peuvent devenir de plus en plus différentes des données d'entraînement avec le temps: on parle de _data drift_ ou de _distribution shift_. Il faut alors réentraîner le modèle sur des données plus récentes. \n",
    "\n",
    "\n",
    "### Question 12\n",
    "Appliquez une PCA pour la réduire et implémentez un test statistique pour comparer les moyennes des embeddings Train et Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f925f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea6f3976",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "En pratique, comparer les moyennes des distributions n'est pas suffisant pour détecter le drift. Si on obtient les vrais labels (même avec un petit retard), alors  on peut calculer les métriques de performances (précision, rappel) au cours du temps. Sinon, on peut chercher à prédire si un échantillon vient du train ou du test ! L'idée est: si un modèle ML est capable de les distinguer: il y a forcément un drift. Trouvez le meilleur modèle capable de le faire en utilisant les embeddings de BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f65a5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f8c015",
   "metadata": {},
   "source": [
    "## V Partie 5: Modélisation non supervisée\n",
    "\n",
    "#### Question 14\n",
    "Sans utiliser les labels, modéliser en utilisant le modèle GMM de `scikit-learn` les données (BERT embeddings + PCA) en utilisant quelques modèles différents (nombre de composantes 2 vs 3 -- et type de covariance). Comparez les performances avec les vrais labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc5024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5bf8af",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "Comparez ces modèles en utilisant le test du rapport de vraisemblance. Quel est le modèle plausible le plus simple ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb29db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
