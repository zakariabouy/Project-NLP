{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf899e45",
   "metadata": {},
   "source": [
    "* * *\n",
    "<pre> INSEA                <i> Projet Statistiques Multivariées - 2025 </i></pre>\n",
    "* * *\n",
    "\n",
    "\n",
    "<pre align=\"left\"> Mardi 4 Novembre  2025             <i> Author: Hicham Janati </i></pre>\n",
    "* * *\n",
    "\n",
    "Consignes:\n",
    "----------\n",
    "- Deadline: *Dimanche 7 Décembre 23h59* | tout retard d'1h = -1\n",
    "- Tout travail doit être effectué par groupe de deux sans exception.\n",
    "- Il faut nommer le fichier avec le numéro du groupe seulement `groupe_X.ipynb` pas de noms. \n",
    "- PAS DE PDF NI DE FICHIER WORD. UN NOTEBOOK UNIQUEMENT.\n",
    "- Les noms des membres du groupe sont à mentionner en modifiant la ligne suivante:\n",
    "\n",
    "Membre 1:\n",
    "\n",
    "Membre 2:\n",
    "\n",
    "- Ce notebook contient 15 questions.\n",
    "- Il faut executer tout le notebook (Run all cells) avant de l'envoyer: vérifier que les cellules sont exécutées dans le bon ordre et qu'il n'y a pas d'erreur, je ne vais pas débugger votre code. \n",
    "- Le code doit être propre et lisible et surtout comprendre ce qu'il fait par tous les membres du groupe.\n",
    "- LIVRAISON UNIQUEMENT SUR DROPBOX (TOUT FICHIER ENVOYE PAR MAIL NE SERA PAS OUVERT):\n",
    "\n",
    "https://www.dropbox.com/request/bs2Tafzm0Bzr7rroAGtc\n",
    "\n",
    "Vous pouvez effectuer plusieurs uploads, seule la date la plus récente sera corrigée.\n",
    "\n",
    "- Le but n'est pas de produire du code qui marche: le but est de comprendre et maîtriser les mécanismes derrière et savoir utiliser quoi et quand. Ce notebook est loin d'être self-contained, cherchez, lisez et apprenez à devenir auto-didacte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ebe82",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f292b97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35d2f782",
   "metadata": {},
   "source": [
    "# Partie 1: Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6afb9",
   "metadata": {},
   "source": [
    "Les données sont coupés en deux: train et test. Il s'agit d'une liste d'SMS avec leur labels (Y) spam / non spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load(\"data/X_train.npy\", allow_pickle=True)\n",
    "X_test = np.load(\"data/X_test.npy\", allow_pickle=True)\n",
    "Y_train = np.load(\"data/Y_train.npy\", allow_pickle=True)\n",
    "Y_test = np.load(\"data/Y_test.npy\", allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:4], Y_train[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5e565",
   "metadata": {},
   "source": [
    "On rappelle que les données de `test` doivent être utilisées pour évaluer le modèle final comme si elles étaient nouvelles en production -- non disponibles à l'entraînement -- toute opération d'apprentissage (y compris la validation croisée) doit être faite sur les données `train` uniquement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3366050",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "En utilisant ```CountVectorizer``` de ```scikit-learn``` transformez les données. Quelle est l'utilité de cette opération ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cb7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a4a093b",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "On modélise les données de chaque classe par une loi multinomiale. En utilisant ```MultinomialNB``` de scikit-learn, fittez un modèle et évaluez sa performance en calculant les scores de précision et de rappel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509eb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b95a3d7d",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "Implémentez votre propre modèle naive bayes multinomial en utilisant numpy. Comparez avec scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dfadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3034ba",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "Afin de comparer avec d'autres modèles qui s'attendent à des inputs continus, on peut utiliser des fréquences de mots au lieu du nombre brut. Appliquez la transformation `TfIdf` de scikit-learn sur les données. Fittez des modèles différents: LDA, régression logistique, QDA et SVM. Essayez de battre la performance du MulinomialNB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8d576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29b781aa",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "Visualisez la PCA et tSNE des données transformées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86c4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c872a753",
   "metadata": {},
   "source": [
    "# Partie II: Introduction au NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47767fe",
   "metadata": {},
   "source": [
    "Dans cette partie on utilise des transformations plus avancées basées sur la factorisation matricielle. L'un des premiers modèles à grand succès pour représenter les mots est `Word2Vec`. Étant donné un vocabulaire de taille V, on peut représenter un input par un vecteur binaire de taille V (one-hot encoding). Par exemple si le vocabulaire est\n",
    "```[\"rouge\", \"chat\", \"souris\", \"courir\", \"mange\", \"ordinateur\", \"la\", \"le\", ...]``` \n",
    "Alors on peut représenter chaque mot par son vecteur one-hot de taille V:\n",
    "- \"chat\" -> [0, 1, 0, 0, 0, ...]\n",
    "- \"ordinateur\" -> [0, 0, 0, 0, 0, 1, 0, ...]\n",
    "\n",
    "La phrase \"le chat mange la souris\" peut être représentée par [0, 1, 1, 0, 1, 0, 1, 1, ...].\n",
    "\n",
    "On construit un réseau de neurones simple capable de prédire le mot manquant au milieu d'une phrase avec des données comme:\n",
    "- \"le chat la souris\" -> \"mange\"\n",
    "- \"la matrice est pas inversible\" -> \"n'\" \n",
    "...\n",
    "Ainsi, l'input du réseau doit être un vecteur de taille V. L'output doit être également **un vecteur de probabilités** de taille V. On prédit le mot avec la plus grande probabilité.\n",
    "\n",
    "On considère un réseau de neurones à une couche cachée de dimension M (M neurones). Le réseau de neurones doit en sortie prédire un mot caché. Soit $x \\in \\mathbb R^V$ un one-hot vecteur. Le réseau de neurones peut être défini formellement comme suit. Soit $W_{in} \\in \\mathbb{R}^{V \\times M}$ et $W_{out} \\in \\mathbb{R}^{M \\times V}$. \n",
    "\n",
    "$$ g(x) = W_{out}(W_{in}x) \\in \\mathbb R^{V} $$\n",
    "\n",
    "Pour obtenir un vecteur output positif et sommant à 1 (des probabilités sur les mots), on applique la fonction softmax:\n",
    "$$ softmax: z \\in \\mathbb R^V \\mapsto \\left[\\frac{e^{z_1}}{\\sum_j e^{z_j}}, \\dots, \\frac{e^{z_V}}{\\sum_j e^{z_j}}  \\right]^\\top$$\n",
    "\n",
    "Ainsi on définit: $ neuralnet(x) = softmax(g(x))$ et on entraîne le modèle avec la loss cross-entropy.\n",
    " \n",
    "Après avoir entraîné le modèle, les ligne de la matrice $W_{in}$ sont les embeddings des mots du vocabulaire.\n",
    "\n",
    "\n",
    "On commence par télécharger le modèle word2vec (peut prendre du temps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3818bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbabe3a",
   "metadata": {},
   "source": [
    "word2vec est n'est qu'un dictionnaire spécial \"mot\" -> vecteur:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b64da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec[\"computer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcecdc8",
   "metadata": {},
   "source": [
    "On peut obtenir l'embedding de plusieurs mots directement dans une liste. L'embedding d'une phrase est souvent défini comme la moyenne des embeddings de ses mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec[[\"computer\", \"mouse\", \"keyboard\"]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a273ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec[[\"I\", \"love\", \"computers\"]].mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6fc08",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Pour appliquer word2vec aux SMS, il faut d'abord les découper en mots. Implémentez une fonction naïve qui transforme un SMS en liste de mots ou _tokens_. C'est ce qu'on appelle un tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a05f7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f73641d4",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "On peut vérifier si un mot fait partie du vocabulaire avec le test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32398c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"computer\" in word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9090ca6",
   "metadata": {},
   "source": [
    "Proposez une solution pour appliquer word2vec aux données SMS. Est-il meilleur comme embedding comparé au `CountVectorizer` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e9ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be090b85",
   "metadata": {},
   "source": [
    "`Word2vec` (2013) est l'un des premiers modèles de représentation vectorielle du langage naturel. L'une de ses limitations principales est le fait de ne pas pouvoir représenter des mots (ou bouts de mots) non-vus lors de l'entraînement. Quelques années plus tard, plusieurs avancées ont été introduites pour y remédier:\n",
    "\n",
    "### 1) Améliorer la tokenization:\n",
    "1. Au lieu de considérer un vocabulaire avec des mots, on considère un vocabulaire avec les mots, et les bouts de mots (ngrams):\n",
    "\n",
    "    \"diagonalisation matricielle\" -> \"diagonal\", \"isation\", \" \", \"matric\", \"ielle\". \n",
    "\n",
    "    Ceci est utilisé dans le modèle `fastText` (2016) dont l'architecture est similaire à celle de Word2Vec.\n",
    "\n",
    "2. WordPiece: approche bottom-up où le vocabulaire des tokens est construit en mergeant les caractères (a-Z, 0-9, symboles et ponctuation) + Ajouter des tokens spéciaux \"[UNK]\" (unknow) pour gérer les tokens non-vus, \"[SEP]\" (séparateur entre phrases ou textes), \"[MASK]\" (token qui veut dire \"ce mot est à prédire\") etc. Ce tokenizer est utilisé dans l'un des premiers modèles basés sur l'architecture Transformer (BERT) (2018).\n",
    "\n",
    "3. Byte-pair encoding (BPE): similaire à WordPiece mais à l'échelle du byte.  Ceci permet d'encoder n'importe quel string dans n'importe quelle langue: mots rares, typos, emojis, kanji... C'est le tokenizer adopté par l'un des fameux modèles de traduction à l'époque (Neural Machine Translation -- NMT 2015) pour traduire les mots rares; et ensuite par les modèles GPT (2018). Avec BPE, on n'a plus besoin du token spécial _unknown_ [UNK]. Vous pouvez voir comment le texte est tokenisé par GPT en visitant https://platform.openai.com/tokenizer. \n",
    "\n",
    "### 2) Améliorer l'architecture\n",
    "\n",
    "1. Limites de Word2Vec et companie:\n",
    "\n",
    "La nature des données de langage est séquentielle (comme les séries temporelles, mot après mot..), or les modèles comme Word2Vec et ses variantes (Fasttext, GloVe) ne prennent pas en considération l'ordre des mots: ils sont appliqués à des _bag of words_. En plus, même si ses modèles utilisent le contexte voisin, après l'entraînement, l'embedding d'un mot est le même quelque soit son contexte ce qui limite la compréhension des différentes nuances d'un même mot. \n",
    "\n",
    "2. Recurrent Neural Networks:\n",
    "\n",
    "L'architecture des réseaux de neurones récurrents (RNN) et ses variantes (LSTM, GRU) développés principalement dans les années 1990 sont revisités en (2013-2020) après le succès du deep learning (2012+) avec la réalisation de la grande puissance de calcul des GPUs. Ils permettent de prendre en considération l'aspect temporel des données mais restent difficiles à entraîner (nécessitent beaucoup de temps + instabilité des gradients). \n",
    "\n",
    "3. Transformers (en très bref):\n",
    "\n",
    "En 2017, l'architecture des Transformers est introduite. Son avantage principal est son formalisme matriciel qui est beaucoup plus parallélisable que les RNNs, et donc permet d'entraîner des modèles plus grands sur des données beaucoup plus grandes. Prenons le cas d'une architecture générative (GPT). Un transformer définit une structure appelée _Self-Attention Head_ qui fonctionne comme suit.\n",
    "Étant donnés les embeddings (statiques comme ceux de Word2Vec) des tokens d'une phrase, le mécanisme d'attention consiste à calculer des scores de similarités (produits scalaires) entre chaque paire d'embeddings, ceci permet d'obtenir une matrice de scores \"d'attention\" entre les tokens.\n",
    "\n",
    "Par exemple, pour une phrase \"Il me faut un tapis pour la souris Logitec\", l'embedding du mot \"souris\" sera pondéré par ses scores de similarité avec chacun des autres mots [\"Il\", \"me\", \"faut\", \"un\", \"tapis\", \"pour\", \"la\", \"logitec\"]. Ainsi, l'embedding d'un même mot dépend du contexte. Ceci permettra de distinguer la souris (animal) de la souris d'ordinateur.\n",
    "\n",
    " Ceci reste une explication très simpliste du mécanisme d'attention, pour visualiser le concept, regardez la vidéo de 3b1b https://www.youtube.com/watch?v=eMlx5fFNoYc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5a415",
   "metadata": {},
   "source": [
    "## III Partie 3 - Embeddings basés sur les transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf35bb",
   "metadata": {},
   "source": [
    "Dans cette partie, on passe à l'artillerie lourde: embeddings basés sur les transformers. On utilise ceux de BERT plutôt que GPT car BERT a été conçu pour la classification supervisée alors que GPT pour la génération de texte: l'embedding de BERT est plus pertinent pour notre problématique.\n",
    "\n",
    "On commence par tokeniser les données. Voici ce que donne le tokenizer avec une phrase par exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4469cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71330dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hey, are you free tomorrow at 8 ? gotta catch up bud.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2aec4",
   "metadata": {},
   "source": [
    "Le modèle BERT en revanche ne s'attend pas à des tokens en texte, mais aux token_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hey, are you free tomorrow at 8 ? gotta catch up bud\"\n",
    "token_ids = tokenizer(text, return_tensors=\"pt\")\n",
    "token_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5213c82",
   "metadata": {},
   "source": [
    "On fait appel ensuite au modèle BERT. On peut voir les composants de son architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()\n",
    "bert_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc07b8ac",
   "metadata": {},
   "source": [
    "On applique le modèle aux token_ids sans calcul de gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482197eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = bert_model(**token_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6611fe03",
   "metadata": {},
   "source": [
    "On peut accéder aux embeddings en cherchant la dernière couche cachée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86436f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389bfc99",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Expliquez la dimension obtenue de ces embeddings. On souhaite obtenir les embeddings de toutes les données directement. Appliquez la tokenization à la liste de textes suivante. Quel problème se pose ? Comment y remédier ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80fc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Hey, are you free tomorrow at 8 ? gotta catch up bud.\", \"Sure thing, let's go.\"]\n",
    "token_ids = tokenizer(texts, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfec6a8",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Après avoir fixé le problème, analysez les tokens obtenus et le `attention_mask` de l'output. Comment peut-on savoir où le `padding` a été appliqué ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669b170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6225300c",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "L'embedding d'une phrase est la moyenne des embeddings de ses tokens. Appliquez BERT à une liste des donnés spams (X_train[:10]). Calculez le temps pris par cette opération en utilisant la librairie `time`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4020d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "713b2079",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "Avec une GPU, ce calcul peut être jusqu'à 100x fois plus rapide. On vous fournit les embeddings des données `X_train` et `X_test` calculés avec BERT. Refaire l'étape de visualisation et classification avec ces embeddings pour comparer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train_bert = np.load(\"data/embeddings_train.npy\")\n",
    "X_test_bert = np.load(\"data/embeddings_test.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b87e89",
   "metadata": {},
   "source": [
    "# IV Partie 4: Data drift \n",
    "En production, les données nouvelles peuvent devenir de plus en plus différentes des données d'entraînement avec le temps: on parle de _data drift_ ou de _distribution shift_. Il faut alors réentraîner le modèle sur des données plus récentes. \n",
    "\n",
    "\n",
    "### Question 12\n",
    "Appliquez une PCA pour la réduire et implémentez un test statistique pour comparer les moyennes des embeddings Train et Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f925f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea6f3976",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "En pratique, comparer les moyennes des distributions n'est pas suffisant pour détecter le drift. Si on obtient les vrais labels (même avec un petit retard), alors  on peut calculer les métriques de performances (précision, rappel) au cours du temps. Sinon, on peut chercher à prédire si un échantillon vient du train ou du test ! L'idée est: si un modèle ML est capable de les distinguer: il y a forcément un drift. Trouvez le meilleur modèle capable de le faire en utilisant les embeddings de BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f65a5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f8c015",
   "metadata": {},
   "source": [
    "## V Partie 5: Modélisation non supervisée\n",
    "\n",
    "#### Question 14\n",
    "Sans utiliser les labels, modéliser en utilisant le modèle GMM de `scikit-learn` les données (BERT embeddings + PCA) en utilisant quelques modèles différents (nombre de composantes 2 vs 3 -- et type de covariance). Comparez les performances avec les vrais labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc5024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5bf8af",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "Comparez ces modèles en utilisant le test du rapport de vraisemblance. Quel est le modèle plausible le plus simple ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb29db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
